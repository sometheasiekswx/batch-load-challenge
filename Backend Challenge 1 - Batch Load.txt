Backend Challenge 1 - Batch Load.txt

Objectives:

Test knowledge in nodejs, mongodb/postgres
Scalability - Approach on code structure and architecture to handle large amount of data
Testing - unit and performance testing approach
Duration: approx 90 minutes

The Challenge:

Write a simple batch job that retrieves a CSV file from a URL, which imports orders into a database. Assume there is an existing collection/table of customers and orders. Ensure that the customerId exists in the database before importing the order, otherwise skip the import for the order.

Note:

timebox it! Please feel free to ask questions! Use version control, preferrably git. Add your approach to testing. Ensure the batch can handle files of varying sizes without having to scale vertically Think production ready batch that can scale with huge data (Gbs of data)

Language:

node.js (and use any library or frameworks that you are comfortable with)

Database:

MongoDB or Postgres

CSV header with a sample input (1 order per line): orderId,customerId,item,quantity sample-123,customer-321,Flowers,2

Table/Collection schema:

Customer: customerId (String) firstName (String) lastName (String) 

Orders: orderId (String) customerId (String) item (String) quantity (Number)